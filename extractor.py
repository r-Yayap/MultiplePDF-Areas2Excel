#extractor.py

import multiprocessing
import os
import re
import shutil
import secrets
import pymupdf as fitz
import sys
import csv
import time
import gc
import json
from openpyxl import Workbook
from openpyxl.styles import Font
from openpyxl.utils import get_column_letter
from datetime import datetime
from openpyxl import Workbook
from openpyxl.drawing.image import Image as ExcelImage
from openpyxl.styles import Font
from openpyxl.utils import get_column_letter
from openpyxl.utils import column_index_from_string
from utils import adjust_coordinates_for_rotation, find_tessdata
import psutil #for debug
import os



# Define patterns
REVISION_REGEX = re.compile(r"^[A-Z]{1,2}\d{1,2}[a-zA-Z]?$", re.IGNORECASE)
DATE_REGEX = re.compile(r"""12
    (?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}) |               # 01/01/24 or 1-1-2025
    (?:\d{1,2}\s*[-]?\s*[A-Za-z]{3,9}\s*[-]?\s*\d{2,4})  # 3-Apr-2025 or 3 April 25
""", re.VERBOSE)
DESC_KEYWORDS = ["issued for", "issue", "submission", "schematic", "detailed", "concept", "design", "construction", "revised", "resubmission"]

def print_ram(): #for debug
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024 / 1024
    print(f"üìä Current RAM usage: {mem_mb:.2f} MB")

def process_single_pdf_standalone(pdf_path, areas, revision_area, ocr_settings, pdf_folder, temp_image_folder, unid):
    extractor = TextExtractor(
        pdf_folder=pdf_folder,
        output_excel_path="",
        areas=areas,
        ocr_settings=ocr_settings,
        include_subfolders=False,
        batch_threshold=0
    )
    extractor.revision_area = revision_area
    extractor.temp_image_folder = temp_image_folder

    result_rows = extractor.process_single_pdf(pdf_path)

    csv_path = os.path.join(temp_image_folder, f"temp_{unid}.csv")
    jsonl_path = os.path.join(temp_image_folder, f"temp_{unid}.ndjson")

    # Stream directly to temp CSV/NDJSON (per worker)
    with open(csv_path, "w", newline="", encoding="utf-8") as csv_file, \
            open(jsonl_path, "w", encoding="utf-8") as jsonl_file:
        csv_writer = csv.writer(csv_file)

        for row in result_rows:
            file_size, mod_date, folder, filename, page_no, areas, revisions = row
            revisions_flat = [f"{r['rev']} | {r['desc']} | {r['date']}" for r in revisions]
            text_values = [text for text, _ in areas]

            # CSV Row
            csv_row = [unid, file_size, mod_date, folder, filename, page_no] + text_values + [revisions_flat]
            csv_writer.writerow(csv_row)

            # NDJSON Row
            json.dump({"unid": unid, "revisions": revisions}, jsonl_file, ensure_ascii=False)
            jsonl_file.write("\n")

    # Explicit cleanup
    extractor = None
    gc.collect()

    return True  # only a small flag indicating success

def _unwrap_process_single_pdf(args):
    return process_single_pdf_standalone(*args)


class TextExtractor:
    def __init__(self, pdf_folder, output_excel_path, areas, ocr_settings, include_subfolders, revision_regex=None, batch_threshold=100):
        self.header_column_map = None
        self.final_output_path = None
        self.pdf_folder = pdf_folder
        self.output_excel_path = output_excel_path
        self.areas = areas
        self.ocr_settings = ocr_settings
        self.include_subfolders = include_subfolders
        self.revision_regex = re.compile(revision_regex, re.IGNORECASE) if revision_regex else REVISION_REGEX
        self.tessdata_folder = None  # Will be set lazily if OCR is needed
        self.batch_threshold = batch_threshold

        # Initialize headers with fixed metadata columns
        self.headers = ["Size (Bytes)", "Date Last Modified", "Folder", "Filename", "Page No"]

        # Dictionary to store unique header assignments per area
        self.unique_headers_mapping = {}  # Maps each rectangle index to a unique column header

        header_count = {}  # To count occurrences of each title

        for i, area in enumerate(self.areas):
            title = area.get("title", f"Area {i + 1}")  # Default title if not manually set

            # Ensure unique title for each rectangle
            if title not in header_count:
                header_count[title] = 1
                unique_title = title
            else:
                header_count[title] += 1
                unique_title = f"{title} ({header_count[title]})"

            self.headers.append(unique_title)  # Add unique header to headers list
            self.unique_headers_mapping[i] = unique_title  # Assign rectangle index to its specific column

        # Define application directory (where the script is located)


        if getattr(sys, 'frozen', False):  # Check if running as PyInstaller EXE
            app_directory = os.path.dirname(sys.executable)  # Use the real EXE location
        else:
            app_directory = os.path.dirname(os.path.abspath(__file__))  # Running as script

        # Create a secure temp folder with random name
        main_temp_folder = os.path.join(app_directory, "temp")
        os.makedirs(main_temp_folder, exist_ok=True)

        # Create random subfolder name (16 random chars)
        random_str = secrets.token_hex(8)
        self.temp_image_folder = os.path.join(main_temp_folder, random_str)

        # Revision Area
        self.revision_area = None  # Will be set externally if needed
        self.unid_counter = 10001


    def detect_column_indices(self, sample_rows, max_rows=3):
        col_scores = {}

        for row in sample_rows[:max_rows]:
            for col_idx, cell in enumerate(row):
                text = str(cell).strip().lower()
                if not text:
                    continue
                if col_idx not in col_scores:
                    col_scores[col_idx] = {"rev": 0, "date": 0, "desc": 0}

                if self.revision_regex.fullmatch(text.upper()):
                    col_scores[col_idx]["rev"] += 1
                elif DATE_REGEX.search(text):
                    col_scores[col_idx]["date"] += 1
                elif any(kw in text for kw in DESC_KEYWORDS):
                    col_scores[col_idx]["desc"] += 1

        rev_idx = max(col_scores.items(), key=lambda x: x[1]["rev"], default=(None, {}))[0]
        desc_idx = max(col_scores.items(), key=lambda x: x[1]["desc"], default=(None, {}))[0]
        date_idx = max(col_scores.items(), key=lambda x: x[1]["date"], default=(None, {}))[0]

        #print(f"üîç Column Indices Detected ‚Üí Rev: {rev_idx}, Desc: {desc_idx}, Date: {date_idx}")
        return rev_idx, desc_idx, date_idx

    def parse_revision_row(self, row, rev_idx, desc_idx, date_idx):
        rev = row[rev_idx].strip() if rev_idx is not None and rev_idx < len(row) else None
        desc = row[desc_idx].strip() if desc_idx is not None and desc_idx < len(row) else None
        date = row[date_idx].strip() if date_idx is not None and date_idx < len(row) else None

        # Validate formats
        if rev and not self.revision_regex.fullmatch(rev.upper()):
            rev = None
        if date and not DATE_REGEX.search(date):
            date = None

        return rev, desc, date

    def extract_revision_history_from_page_obj(self, page, revision_coordinates):
        try:
            if not revision_coordinates or len(revision_coordinates) != 4:
                return []

            clip_rect = fitz.Rect(revision_coordinates)
            if clip_rect.is_empty or clip_rect.get_area() < 100:
                print(f"‚ö†Ô∏è Revision area too small or empty, skipping.")
                return []

            tables = page.find_tables(clip=clip_rect)
            if not tables or not tables.tables:
                print("‚ùå No tables found.")
                return []

            table = tables.tables[0]
            data = table.extract()
            if not data or len(data) < 2:
                print("‚ùå Not enough rows to detect header or parse data.")
                return []

            data = data[::-1]
            rev_idx, desc_idx, date_idx = self.detect_column_indices(data[1:])
            extracted_list = []

            for row in data:
                if not any(row): continue
                rev, desc, date = self.parse_revision_row(row, rev_idx, desc_idx, date_idx)
                if rev and desc and date:
                    extracted_list.append({"rev": rev, "desc": desc, "date": date})
                else:
                    print(f"‚õî Skipped incomplete row: {row}")

            return extracted_list

        except Exception as e:
            print(f"‚ùå Error during revision extraction: {e}")
            return []

    def clean_text(self, text):
        """Cleans text by replacing newlines, stripping, and removing illegal characters."""
        replacement_char = '‚ñ†'  # Character to replace prohibited control characters

        # Step 1: Replace newline and carriage return characters with a space
        text = text.replace('\n', ' ').replace('\r', ' ')

        # Step 2: Strip leading and trailing whitespace
        text = text.strip()

        # Step 3: Replace prohibited control characters with a replacement character
        text = re.sub(r'[\x00-\x1F\x7F-\x9F]', replacement_char, text)

        # Step 4: Remove extra spaces between words
        return re.sub(r'\s+', ' ', text)

    def stream_to_excel(self, csv_path, final_output_path, max_revisions):
        wb = Workbook()
        ws = wb.active

        base_headers = ["Size (Bytes)", "Date Last Modified", "Folder", "Filename", "Page No"]
        area_headers = [self.unique_headers_mapping[i] for i in range(len(self.areas))]
        revision_headers = [f"Rev{i + 1}" for i in range(max_revisions)]
        headers = ["UNID"] + base_headers + area_headers + revision_headers
        ws.append(headers)

        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            next(reader)  # skip header row

            ocr_font = Font(color="FF3300")
            area_start_col = 1 + len(["UNID"] + base_headers)

            for row in reader:
                unid = row[0]
                base = row[1:6]
                areas = row[6:-1]
                revisions = eval(row[-1]) if row[-1].startswith("[") else []

                row_data = [unid] + base + areas + revisions + [""] * (max_revisions - len(revisions))
                ws.append(row_data)
                current_row = ws.max_row

                # Apply hyperlink styling to filename cell
                folder_col_idx = headers.index("Folder") + 1
                filename_col_idx = headers.index("Filename") + 1

                folder_cell = ws.cell(row=current_row, column=folder_col_idx)
                filename_cell = ws.cell(row=current_row, column=filename_col_idx)

                if folder_cell.value and filename_cell.value:
                    abs_path = os.path.abspath(os.path.join(self.pdf_folder, folder_cell.value, filename_cell.value))
                    filename_cell.hyperlink = abs_path
                    filename_cell.font = Font(color="0000FF", underline="single")

                # ‚úÖ OCR styling + image embedding per cell
                for i in range(len(area_headers)):
                    cell = ws.cell(row=current_row, column=area_start_col + i)

                    if isinstance(cell.value, str) and "_OCR_" in cell.value:
                        cell.font = ocr_font
                        cell.value = cell.value.replace("_OCR_", "").strip()

                    if self.ocr_settings["enable_ocr"] == "Text1st+Image-beta":
                        folder = base[2]  # Folder
                        filename = base[3]  # Filename
                        page_no = base[4]  # Page No

                        image_filename = f"{filename}_page{page_no}_area{i}.png"
                        image_path = os.path.join(self.temp_image_folder, image_filename)

                        if os.path.exists(image_path):
                            try:
                                img = ExcelImage(image_path)
                                img.anchor = f"{get_column_letter(area_start_col + i)}{current_row}"
                                ws.add_image(img)
                            except Exception as e:
                                print(f"‚ö†Ô∏è Failed to embed image {image_filename}: {e}")


        # Save file
        if os.path.exists(self.output_excel_path):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name, file_ext = os.path.splitext(self.output_excel_path)
            output_path = f"{file_name}_{timestamp}{file_ext}"
        else:
            output_path = self.output_excel_path

        wb.save(output_path)
        final_output_path.value = output_path

        # ‚úÖ Copy NDJSON from temp folder to match Excel path
        ndjson_temp_path = os.path.join(self.temp_image_folder, "streamed_revisions.ndjson")
        ndjson_output_path = output_path.replace(".xlsx", "_revisions.ndjson")

        try:
            if os.path.exists(ndjson_temp_path):
                shutil.copy(ndjson_temp_path, ndjson_output_path)
                #print(f"‚úÖ NDJSON revision file saved ‚Üí {ndjson_output_path}")
            else:
                print(f"‚ö†Ô∏è NDJSON temp file not found at {ndjson_temp_path}")
        except Exception as e:
            print(f"‚ùå Failed to copy NDJSON file: {e}")

        print(f"‚úÖ Excel saved with hyperlinks ‚Üí {output_path}")

    def combine_temp_files(self, final_output_path):
        import glob

        temp_csv_files = sorted(glob.glob(os.path.join(self.temp_image_folder, "temp_*.csv")))
        temp_jsonl_files = sorted(glob.glob(os.path.join(self.temp_image_folder, "temp_*.ndjson")))

        combined_csv = os.path.join(self.temp_image_folder, "streamed_output.csv")
        combined_jsonl = os.path.join(self.temp_image_folder, "streamed_revisions.ndjson")

        # Combine CSV
        with open(combined_csv, "w", newline="", encoding="utf-8") as outfile:
            writer = csv.writer(outfile)
            writer.writerow(["UNID", "Size (Bytes)", "Date Last Modified", "Folder", "Filename", "Page No"] +
                            [self.unique_headers_mapping[i] for i in range(len(self.areas))] + ["__revisions__"])

            for f in temp_csv_files:
                with open(f, "r", encoding="utf-8") as infile:
                    reader = csv.reader(infile)
                    writer.writerows(reader)
                os.remove(f)  # Cleanup immediately

        # Combine NDJSON
        with open(combined_jsonl, "w", encoding="utf-8") as outfile:
            for f in temp_jsonl_files:
                with open(f, "r", encoding="utf-8") as infile:
                    shutil.copyfileobj(infile, outfile)
                os.remove(f)  # Cleanup immediately

        # Stream to Excel (existing efficient method)
        max_revisions = self.get_max_revisions(combined_csv)
        self.stream_to_excel(combined_csv, final_output_path, max_revisions)

    def get_max_revisions(self, csv_path):
        max_revs = 0
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            next(reader)
            for row in reader:
                revisions = eval(row[-1]) if row[-1].startswith("[") else []
                max_revs = max(max_revs, len(revisions))
        return max_revs

    def start_extraction(self, progress_counter, total_files, final_output_path):
        n_workers = multiprocessing.cpu_count()
        pdf_files = self.get_pdf_files()
        total_files.value = len(pdf_files)

        os.makedirs(self.temp_image_folder, exist_ok=True)

        jobs = [
            (pdf_path, self.areas, self.revision_area, self.ocr_settings, self.pdf_folder, self.temp_image_folder,
             str(10000 + idx))
            for idx, pdf_path in enumerate(pdf_files)
        ]

        with multiprocessing.Pool(processes=n_workers) as pool:
            for _ in pool.imap_unordered(_unwrap_process_single_pdf, jobs):
                with progress_counter.get_lock():
                    progress_counter.value += 1
                    if progress_counter.value % 50 == 0:
                        print_ram()

        # After multiprocessing ends, combine CSV/NDJSON files quickly
        self.combine_temp_files(final_output_path)



    def process_single_pdf(self, pdf_path):
        """Processes a single PDF file, extracting text and images as necessary."""
        try:
            # Ensure the file exists and is not empty
            if not os.path.exists(pdf_path) or os.path.getsize(pdf_path) == 0:
                print(f"Skipping missing or empty file: {pdf_path}")
                result_rows = [["Error", "File not found or empty", "", pdf_path, "",
                                "FileNotFoundError: File not found or empty"]]


                return result_rows

            # Open the PDF file with additional validation and caching
            try:
                # Use memory-efficient loading for large files
                pdf_document = fitz.open(pdf_path, filetype="pdf")
                if not pdf_document.is_pdf:
                    raise ValueError("Not a valid PDF file")

                # Enable faster text extraction
                fitz.TOOLS.set_small_glyph_heights(True)
            except Exception as e:
                print(f"Error opening PDF {pdf_path}: {e}")
                result_rows = [["Error", "Invalid PDF", "", pdf_path, "",
                                f"PDFError: {str(e)}"]]


                return result_rows
            file_size = os.path.getsize(pdf_path)
            last_modified_date = datetime.fromtimestamp(os.path.getmtime(pdf_path)).strftime('%Y-%m-%d %H:%M:%S')
            folder = os.path.relpath(os.path.dirname(pdf_path), self.pdf_folder)
            filename = os.path.basename(pdf_path)

            result_rows = []  # Collect each page's data here

            # Process each page safely
            for page_number in range(pdf_document.page_count):
                try:
                    page = pdf_document[page_number]
                    page.remove_rotation()
                    extracted_areas = []

                    for area_index, area in enumerate(self.areas):
                        coordinates = area["coordinates"]
                        text_area, img_path = self.extract_text_from_area(page, coordinates, pdf_path, page_number,
                                                                          area_index)

                        # If text_area is empty, treat it as blank rather than "Error"
                        extracted_areas.append((text_area if text_area != "Error" else "", img_path or ""))

                    revision_data = []
                    if self.revision_area:
                        revision_data = self.extract_revision_history_from_page_obj(
                            page, self.revision_area["coordinates"]
                        )

                    result_rows.append([
                        file_size, last_modified_date, folder, filename, page_number + 1,
                        extracted_areas, revision_data  # No UNID here
                    ])

                    del extracted_areas
                    del revision_data
                    del page
                    gc.collect()  # ‚úÖ Force garbage collection

                except Exception as e:
                    print(f"Error processing page {page_number + 1} in {pdf_path}: {e}")
                    result_rows.append(
                        [file_size, last_modified_date, folder, filename, page_number + 1, "Error processing page",
                         str(e)])

            pdf_document.close()
            del pdf_document
            gc.collect()

            return result_rows

        except Exception as e:
            print(f"Error processing {pdf_path}: {e}")
            # Add an error placeholder for this file in results
            result_rows = [
                [file_size, last_modified_date, folder, filename, "Error", f"File Processing Error: {str(e)}"]]

            gc.collect()
            # total_files.value += 1  # Update progress counter even on error
            return result_rows

    def get_pdf_files(self):
        """Gathers all PDF files within the specified folder with validation."""
        pdf_files = []

        # Validate base folder exists and is accessible
        if not os.path.exists(self.pdf_folder):
            raise ValueError(f"PDF folder does not exist: {self.pdf_folder}")
        if not os.path.isdir(self.pdf_folder):
            raise ValueError(f"PDF path is not a directory: {self.pdf_folder}")

        for root_folder, subfolders, files in os.walk(self.pdf_folder):
            if not self.include_subfolders:
                subfolders.clear()

            for f in files:
                if f.lower().endswith('.pdf'):
                    full_path = os.path.join(root_folder, f)
                    try:
                        # Validate path components
                        if not os.path.normpath(full_path).startswith(os.path.normpath(self.pdf_folder)):
                            continue  # Skip paths that try to traverse outside base folder
                        if any(part.startswith('.') for part in full_path.split(os.sep)):
                            continue  # Skip hidden files/directories
                        if not os.path.isfile(full_path):
                            continue  # Skip non-files

                        pdf_files.append(full_path)
                    except Exception as e:
                        print(f"Skipping invalid PDF path {full_path}: {e}")

        return pdf_files

    def extract_text_from_area(self, page, area_coordinates, pdf_path, page_number, area_index):
        """Extracts text or image content from a specified area in a PDF page."""
        adjusted_coordinates = adjust_coordinates_for_rotation(
            area_coordinates, page.rotation, page.rect.height, page.rect.width
        )
        text_area = ""
        img_path = None

        try:
            # Text extraction without OCR
            if self.ocr_settings["enable_ocr"] == "Off":
                text_area = page.get_text("text", clip=adjusted_coordinates)

            # OCR only if there's no text
            elif self.ocr_settings["enable_ocr"] == "Text-first":
                text_area = page.get_text("text", clip=adjusted_coordinates)
                if not text_area.strip():  # Perform OCR only if text_area is empty
                    text_area, _ = self.apply_ocr(page, area_coordinates, pdf_path, page_number, area_index)

            # OCR for all areas, no images saved
            elif self.ocr_settings["enable_ocr"] == "OCR-All":
                text_area, _ = self.apply_ocr(page, area_coordinates, pdf_path, page_number, area_index)

            # OCR with image saving regardless of extracted text for Text1st+Image-beta
            elif self.ocr_settings["enable_ocr"] == "Text1st+Image-beta":
                text_area = page.get_text("text", clip=adjusted_coordinates)
                pix = page.get_pixmap(clip=area_coordinates, dpi=self.ocr_settings.get("dpi_value", 150))

                if not os.path.exists(self.temp_image_folder):
                    os.makedirs(self.temp_image_folder, exist_ok=True)

                # Validate and save image
                img_path = os.path.join(self.temp_image_folder, f"{os.path.basename(pdf_path)}_page{page_number + 1}_area{area_index}.png")

                # Ensure temp folder exists
                if not os.path.exists(self.temp_image_folder):
                    os.makedirs(self.temp_image_folder, exist_ok=True)

                # Save image with size validation (20MB max)
                pix.save(img_path)
                if os.path.getsize(img_path) > 10 * 1024 * 1024:  # 20MB
                    os.remove(img_path)
                    img_path = None
                    print(f"Warning: Skipped large image (>10MB) from {pdf_path} page {page_number + 1}")

                # Apply OCR if no text found
                if not text_area.strip():
                    try:
                        text_area, _ = self.apply_ocr(page, area_coordinates, pdf_path, page_number, area_index)
                    except Exception as e:
                        print(f"OCR failed for {pdf_path} page {page_number + 1}: {e}")
                        text_area = "OCR_ERROR"

            # Clean the extracted text
            text_area = self.clean_text(text_area)

            # Return empty string if text_area is blank after cleaning
            return (text_area if text_area.strip() else "", img_path)

        except fitz.EmptyFileError as e:
            print(f"Error: {pdf_path} is empty or corrupted: {e}")
            return f"EmptyFileError: {str(e)}", None

        except fitz.FileNotFoundError as e:
            print(f"Error: {pdf_path} not found: {e}")
            return f"FileNotFoundError: {str(e)}", None

        except RuntimeError as e:
            print(f"Runtime error on page {page_number + 1} in {pdf_path}: {e}")
            return f"RuntimeError: {str(e)}", None

        except Exception as e:
            print(f"Unexpected error extracting text from area {area_index} in {pdf_path}, Page {page_number + 1}: {e}")
            return f"UnexpectedError: {str(e)}", None

    def apply_ocr(self, page, coordinates, pdf_path, page_number, area_index):
        """Applies OCR on a specified area and returns the extracted text and image path."""
        if self.tessdata_folder is None:
            self.tessdata_folder = find_tessdata()
            if not self.tessdata_folder:
                print("‚ùå Tessdata folder not found. OCR cannot proceed.")
                return "", ""

        if not os.path.exists(self.temp_image_folder):
             os.makedirs(self.temp_image_folder, exist_ok=True)

        pix = page.get_pixmap(clip=coordinates, dpi=self.ocr_settings.get("dpi_value", 150))
        pdfdata = pix.pdfocr_tobytes(language="eng", tessdata=self.tessdata_folder)
        clipdoc = fitz.open("pdf", pdfdata)
        text_area = "_OCR_" + clipdoc[0].get_text()
        clipdoc.close()
        del clipdoc
        gc.collect()
        img_path = os.path.join(self.temp_image_folder,f"{os.path.basename(pdf_path)}_page{page_number + 1}_area{area_index}.png")
        pix.save(img_path)
        del pix
        return text_area, img_path

    def extract_revision_history_from_page(self, pdf_path, page, filename, page_number):
        try:
            if (
                    self.revision_area and
                    isinstance(self.revision_area, dict) and
                    "coordinates" in self.revision_area and
                    isinstance(self.revision_area["coordinates"], list) and
                    len(self.revision_area["coordinates"]) == 4
            ):

                revision_data = self.extract_revision_history_from_page_obj(page, self.revision_area["coordinates"])
                self.revision_data_mapping[(filename, page_number + 1)] = revision_data
            else:
                print(f"‚ö†Ô∏è Invalid revision area format for {filename} Page {page_number + 1}: {self.revision_area}")
        except Exception as e:
            print(f"‚ö†Ô∏è Revision extraction failed for {filename} Page {page_number + 1}: {e}")

    def consolidate_results(self, results, final_output_path):
        """Consolidates all extracted results into an Excel file with each page as a row."""
        wb = Workbook()
        ws = wb.active

        # Determine the maximum number of revision entries
        max_revision_columns = max(
            (len(row[6]) for result_pages in results for row in result_pages if
             isinstance(row, list) and len(row) == 7),
            default=0
        )

        # Extend headers dynamically with Rev1, Rev2, ...
        rev_headers = [f"Rev{i + 1}" for i in range(max_revision_columns)]
        self.headers.extend(rev_headers)

        self.header_column_map = {title: idx + 1 for idx, title in enumerate(self.headers)}

        ws.append(self.headers)  # Add headers to the first row

        # Precompute revision column indices to avoid repeated lookups
        rev_col_indices = {
            i: self.header_column_map[header]
            for i, header in enumerate(rev_headers)
        }

        for result_pages in results:
            for row_data in result_pages:
                try:
                    # if not isinstance(row_data, list) or len(row_data) != 7:
                    #     print(f"‚ö†Ô∏è Skipping malformed row_data: {row_data}")
                    #     continue

                    # Unpack metadata and extracted area data for the page
                    file_size, last_modified, folder, filename, page_number, extracted_areas, revision_data = row_data

                    # Prepare row for insertion
                    row_index = ws.max_row + 1
                    metadata = [file_size, last_modified, folder, filename, page_number]

                    # Write metadata to columns 1-5
                    for col, data in enumerate(metadata, start=1):
                        cell = ws.cell(row=row_index, column=col)
                        cell.value = data
                        if col == 4:  # Set filename as a hyperlink in column 4
                            absolute_path = os.path.abspath(os.path.join(self.pdf_folder, folder, filename))
                            cell.hyperlink = absolute_path
                            cell.style = "Hyperlink"

                    # Write extracted areas to columns starting from 6
                    for index, area in enumerate(self.areas):  # Use the updated self.areas list

                        # Get the unique title assigned to this rectangle
                        column_title = self.unique_headers_mapping.get(index, f"Area {index + 1}")
                        col_index = self.header_column_map[column_title]

                        # Retrieve text and image path from extracted areas
                        text, img_path = extracted_areas[index] if isinstance(extracted_areas, list) else (
                        extracted_areas, None)

                        # Place text in the designated column, checking for OCR and cleaning
                        text_cell = ws.cell(row=row_index, column=col_index)
                        cleaned_text = self.clean_text(text.replace("_OCR_", "")) if text != "Error" else "Error"
                        text_cell.value = cleaned_text

                        # Highlight OCR text in red if "_OCR_" was detected
                        if "_OCR_" in text:
                            text_cell.font = Font(color="FF3300")

                        # If there is an associated image, add it to the cell
                        if img_path and img_path != "Error":
                            img = ExcelImage(img_path)
                            img.anchor = f"{get_column_letter(col_index)}{row_index}"
                            ws.add_image(img)

                    # Append revisions if available
                    for i, revision_text in enumerate(revision_data):
                        if i in rev_col_indices:
                            col_index = rev_col_indices[i]
                            ws.cell(row=row_index, column=col_index).value = revision_text

                except Exception as e:
                    print(f"‚ùå Unexpected error: {e}")
                    print(f"   ‚Ü≥ row_data was: {row_data!r}")

                    # pull metadata _directly_ out of this row_data (if possible)
                    size, mod, fld, fn, pg = (row_data + [None] * 5)[:5]

                    # reserve the next row
                    row_index = ws.max_row + 1

                    # rewrite metadata
                    for col, data in enumerate([size, mod, fld, fn, pg], start=1):
                        cell = ws.cell(row=row_index, column=col)
                        cell.value = data
                        if col == 4 and fn:
                            path = os.path.abspath(os.path.join(self.pdf_folder, fld or "", fn))
                            cell.hyperlink = path
                            cell.style = "Hyperlink"

                    # put the error message in column6
                    ws.cell(row=row_index, column=6).value = f"Error: {e}"

                    # blank out the rest of the area columns
                    for c in range(7, 7 + len(self.areas)):
                        ws.cell(row=row_index, column=c).value = ""

        # Generate a unique filename if the output file already exists
        output_filename = self.output_excel_path
        if os.path.exists(output_filename):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name, file_ext = os.path.splitext(output_filename)
            output_filename = f"{file_name}_{timestamp}{file_ext}"

        # Save to the new filename
        try:
            wb.save(output_filename)
            wb.close()
            final_output_path.value = output_filename  # ‚úÖ Save to shared multiprocessing Value

            print(f"Consolidated results saved to {output_filename}")
        except Exception as e:
            print(f"Error saving Excel file: {e}")

        # Secure cleanup of temporary images
        if os.path.exists(self.temp_image_folder):
            try:
                print(f"Deleting temp folder: {self.temp_image_folder}")
                shutil.rmtree(self.temp_image_folder)
            except Exception as e:
                print(f"Warning: Failed to delete temp folder: {e}")
                # Try again on next run
